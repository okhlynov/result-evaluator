#!/usr/bin/env bash
# Ollama LLM Configuration for llm_judge Tutorial
#
# This file configures the LLM client to use a local Ollama instance
# for semantic validation with the llm_judge operator.
#
# Prerequisites:
#   1. Install Ollama from https://ollama.ai
#   2. Pull the model: ollama pull llama3.2
#   3. Start Ollama: ollama serve (or ensure it's already running)
#
# Usage:
#   source tutorial/ollama.env
#
# After sourcing, you'll see confirmation that variables are set.

# ==============================================================================
# Required Configuration
# ==============================================================================

# Ollama API endpoint (local instance default)
# This is the OpenAI-compatible API endpoint that Ollama exposes
export JUDGE_LLM_ENDPOINT=http://localhost:11434/v1

# Model name (ensure you've pulled this model with: ollama pull llama3.2)
# You can use any Ollama model, but llama3.2 is recommended for balance
# of speed and accuracy deepseek-v3.2:cloud
#export JUDGE_LLM_MODEL=deepseek-v3.2:cloud
export JUDGE_LLM_MODEL=gpt-oss:20b
# API key (Ollama doesn't validate this, but the OpenAI client requires a value)
# Can be any non-empty string for Ollama
export JUDGE_LLM_API_KEY=ollama

# ==============================================================================
# Optional Configuration
# ==============================================================================

# Request timeout in seconds (default: 60)
# Increase this if you experience timeout errors with slower models or
# complex semantic validation prompts
export JUDGE_LLM_TIMEOUT=60

# Maximum tokens to generate (default: model default)
# Uncomment to limit response length
# export JUDGE_LLM_MAX_TOKENS=500

# ==============================================================================
# Verification
# ==============================================================================

echo ""
echo "=========================================="
echo " LLM Configuration Loaded"
echo "=========================================="
echo "Endpoint:  $JUDGE_LLM_ENDPOINT"
echo "Model:     $JUDGE_LLM_MODEL"
echo "Timeout:   ${JUDGE_LLM_TIMEOUT}s"
echo "=========================================="
echo ""
echo "Next steps:"
echo "  1. Ensure Ollama is running: ollama serve"
echo "  2. Run tutorial: ./tutorial/run_tutorial.sh"
echo "  3. Or run tests: uv run pytest tutorial/ -v"
echo ""
echo "Debug mode (view LLM prompts):"
echo "  python tutorial/test_with_logging.py"
echo "  cat tutorial/llm_prompts.log"
echo ""
